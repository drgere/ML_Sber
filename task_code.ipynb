{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e573df01",
      "metadata": {
        "id": "e573df01"
      },
      "source": [
        "# 1. Посмотрим на данные"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dddb5ea",
      "metadata": {
        "id": "1dddb5ea"
      },
      "source": [
        "План:\n",
        "\n",
        "1) узнаем число пользователей в обучающей и тестовой выборках  \n",
        "2) узнаем общее число рейтингов в train/test  \n",
        "3) узнаем число уникальных ID треков в обучающей и тестовой выборках  \n",
        "4) проверим, нет ли в тесте ID неизвестных треков  \n",
        "5) рассмотрим распределение числа треков в истории пользователей  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "003b4fdd",
      "metadata": {
        "id": "003b4fdd"
      },
      "outputs": [],
      "source": [
        "# 1) узнаем число пользователей в обучающей и тестовой выборках\n",
        "# 2) узнаем общее число рейтингов в train/test  \n",
        "# 3) узнаем число уникальных ID треков в обучающей и тестовой выборках\n",
        "\n",
        "train_ratings_cnt = 0\n",
        "train_tracks = set()\n",
        "\n",
        "with open('train', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    for l in lines:\n",
        "        for x in l.split():\n",
        "            train_tracks.add(int(x))\n",
        "            train_ratings_cnt += 1\n",
        "\n",
        "print('Train unique tracks:', len(train_tracks))\n",
        "print('Train unique users:', len(lines))\n",
        "print('Train ratings count:', train_ratings_cnt)\n",
        "\n",
        "print()\n",
        "\n",
        "test_ratings_cnt = 0\n",
        "test_tracks = set()\n",
        "\n",
        "with open('test', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    for l in lines:\n",
        "        for x in l.strip().split():\n",
        "            test_tracks.add(int(x))\n",
        "            test_ratings_cnt += 1\n",
        "\n",
        "print('Test unique tracks:', len(test_tracks))\n",
        "print('Test unique users:', len(lines))\n",
        "print('Test ratings count:', test_ratings_cnt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11bd5b89",
      "metadata": {
        "id": "11bd5b89"
      },
      "outputs": [],
      "source": [
        "# 4) проверим, нет ли в тесте ID неизвестных треков\n",
        "\n",
        "print('Len of train_tracks:', len(train_tracks))\n",
        "print('Max track id:', max(train_tracks))\n",
        "print()\n",
        "\n",
        "print('Len of test_tracks:', len(test_tracks))\n",
        "print('Max track id:', max(test_tracks))\n",
        "print()\n",
        "\n",
        "# проверим это явно, чтобы удостовериться\n",
        "\n",
        "warn = False\n",
        "for x in test_tracks:\n",
        "    if x not in train_tracks:\n",
        "        warn = True\n",
        "        break\n",
        "\n",
        "if warn:\n",
        "    print('Detected test track that is not in train')\n",
        "else:\n",
        "    print('OK')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9761abd8",
      "metadata": {
        "id": "9761abd8"
      },
      "outputs": [],
      "source": [
        "# 5) рассмотрим распределение числа треков в истории пользователей\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "tracks_per_user = []\n",
        "\n",
        "with open('train', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    for l in lines:\n",
        "        tracks_per_user.append(len(l.strip().split()))\n",
        "        \n",
        "\n",
        "tracks_per_user = np.array(tracks_per_user)\n",
        "print(\"(Train) Max tracks in user's history:\", max(tracks_per_user))\n",
        "print(\"(Train) Min tracks in user's history:\", min(tracks_per_user))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcd0778b",
      "metadata": {
        "id": "dcd0778b"
      },
      "outputs": [],
      "source": [
        "plt.subplots(1, 2, figsize=(12,6))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.title('(Train) Users w.r.t. tracks in history')\n",
        "plt.xlabel('Tracks')\n",
        "plt.ylabel('Users')\n",
        "\n",
        "plt.hist(\n",
        "    tracks_per_user,\n",
        "    bins=20\n",
        ");\n",
        "\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title('(Train) Users w.r.t. tracks in history (lower counts, detailed)')\n",
        "plt.xlabel('Tracks')\n",
        "plt.ylabel('Users')\n",
        "\n",
        "plt.hist(\n",
        "    tracks_per_user[tracks_per_user < 50],\n",
        "    bins=50\n",
        ");\n",
        "\n",
        "\n",
        "plt.tight_layout();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44520c13",
      "metadata": {
        "id": "44520c13"
      },
      "outputs": [],
      "source": [
        "# 4) рассмотрим распределение числа треков в истории пользователей\n",
        "\n",
        "tracks_per_user_test = []\n",
        "\n",
        "with open('test', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    for l in lines:\n",
        "        tracks_per_user_test.append(len(l.strip().split()))\n",
        "        \n",
        "\n",
        "tracks_per_user_test = np.array(tracks_per_user_test)\n",
        "print(\"(Test) Max tracks in user's history:\", max(tracks_per_user_test))\n",
        "print(\"(Test) Min tracks in user's history:\", min(tracks_per_user_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb620a51",
      "metadata": {
        "id": "eb620a51"
      },
      "outputs": [],
      "source": [
        "plt.subplots(1, 2, figsize=(12,6))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.title('(Test) Users w.r.t. tracks in history')\n",
        "plt.xlabel('Tracks')\n",
        "plt.ylabel('Users')\n",
        "\n",
        "plt.hist(\n",
        "    tracks_per_user_test,\n",
        "    bins=20\n",
        ");\n",
        "\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title('(Test) Users w.r.t. tracks in history (lower counts, detailed)')\n",
        "plt.xlabel('Tracks')\n",
        "plt.ylabel('Users')\n",
        "\n",
        "plt.hist(\n",
        "    tracks_per_user_test[tracks_per_user_test < 50],\n",
        "    bins=50\n",
        ");\n",
        "\n",
        "\n",
        "plt.tight_layout();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdc49e17",
      "metadata": {
        "id": "bdc49e17"
      },
      "source": [
        "Выводы: \n",
        "\n",
        "4) рассмотрим распределение числа треков в истории пользователей \n",
        "\n",
        "1) уникальных пользователей около 1 млн. 200 тыс. в обучении, 300 тыс. в тесте  \n",
        "2) уникальных треков около 500 тысяч обеих выборках  \n",
        "3) всего около 117 млн. рейтингов в train, test  \n",
        "4) причем в тестовой выборке не встретилось неизвестных из обучения треков  \n",
        "5) в истории треков у каждого пользователя в обучении не меньше 7 и не больше 256 треков,\n",
        "   в тесте не меньше 6 и не больше 255 треков  \n",
        "6) при этом распределения числа треков в истории пользователей в обучении и тесте выглядят \n",
        "   крайне похожими"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d081200",
      "metadata": {
        "id": "3d081200"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "e81aea39",
      "metadata": {
        "id": "e81aea39"
      },
      "source": [
        "# 2. Класс для работы с данными"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d6a1116",
      "metadata": {
        "id": "3d6a1116"
      },
      "outputs": [],
      "source": [
        "import scipy.sparse as sp\n",
        "import numpy as np\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "\n",
        "class Dataset(object):\n",
        "    ''' \n",
        "        Класс для обработки обучающей и тестовой выборок\n",
        "        \n",
        "        1) нумерует всех пользователей из train, test\n",
        "        2) собирает инфо о них в user-item матрицу\n",
        "        3) выделяет из train часть для валидации\n",
        "        \n",
        "    '''\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "        self.val_size = None\n",
        "        self.K = None\n",
        "        self.train_users = None\n",
        "        self.test_users = None\n",
        "        self.num_items = None\n",
        "        self.train_matr = None\n",
        "        self.val_ratings = None\n",
        "        self.val_negative_items = None\n",
        "                \n",
        "    def get_train_matr(self):\n",
        "        self.train_matr = self.load_train_positives_matr()\n",
        "    \n",
        "    def set_val_data(self, val_size=0.05, K=99):\n",
        "        self.val_size = val_size\n",
        "        self.K = K\n",
        "        self.val_ratings, self.val_negative_items = self.create_val(val_size, K)\n",
        "    \n",
        "    \n",
        "    def count_train_test_users(self):\n",
        "        num_items = 0\n",
        "        train_users = 0\n",
        "        test_users = 0\n",
        "        \n",
        "        with open(self.path + 'train', 'r') as f:\n",
        "            for l in tqdm(f, desc='Count train users'):\n",
        "                train_users += 1\n",
        "                for i in l.split():\n",
        "                    if num_items < int(i):\n",
        "                        num_items = int(i)\n",
        "        \n",
        "        with open(self.path + 'test', 'r') as f:\n",
        "            for l in tqdm(f, desc='Count test users'):\n",
        "                test_users += 1\n",
        "                for i in l.split():\n",
        "                    if num_items < int(i):\n",
        "                        num_items = int(i)\n",
        "        \n",
        "        self.num_items = num_items + 1   # 0, 1, ..., num_items-1\n",
        "        self.train_users = train_users\n",
        "        self.test_users = test_users\n",
        "        \n",
        "        \n",
        "    \n",
        "    def load_train_positives_matr(self):\n",
        "        mat = sp.dok_matrix(\n",
        "            (self.train_users + self.test_users, self.num_items), \n",
        "            dtype=np.float16\n",
        "        )\n",
        "        \n",
        "        with open(self.path + 'train', 'r') as f:\n",
        "            u = 0\n",
        "            for l in tqdm(f, desc=f'Collect train info, {self.train_users} lines'):\n",
        "                for i in l.split():\n",
        "                    mat[u, int(i)] = 1.0\n",
        "                u += 1\n",
        "                \n",
        "        with open(self.path + 'test', 'r') as f:\n",
        "            u = 0            \n",
        "            for l in tqdm(f, desc=f'Collect test info, {self.test_users} lines'):\n",
        "                for i in l.split():\n",
        "                    mat[u, int(i)] = 1.0\n",
        "                u += 1\n",
        "                \n",
        "        return mat\n",
        "    \n",
        "\n",
        "    def create_val(self, val_size, K):\n",
        "        if type(val_size) is int and val_size < 30:\n",
        "            percent = val_size / 100\n",
        "        elif type(val_size) is float:\n",
        "            percent = val_size\n",
        "        else:\n",
        "            percent = -1\n",
        "        \n",
        "        if percent != -1:\n",
        "            val_size = int(self.train_users * percent)\n",
        "        \n",
        "        users_idxs = np.random.choice(range(self.train_users), size=val_size, replace=False)\n",
        "        users_idxs = set(users_idxs)\n",
        "        \n",
        "        # val_pos_pairs = np.empty((val_size,2))\n",
        "        # val_negative_items = np.empty((val_size, K))\n",
        "        val_pos_pairs = [-1] * val_size\n",
        "        val_negative_items = [-1] * val_size\n",
        "        \n",
        "#         pbar = tqdm(users_idxs, desc='Iterate over val users')\n",
        "#         matr = self.train_matr\n",
        "#         for i, u in enumerate(pbar):\n",
        "#             rated_items_idxs = matr[u].nonzero()[1]\n",
        "#             item = np.random.choice(rated_items_idxs)\n",
        "            \n",
        "#             matr[u, item] = 0.0 # \"забыли\" о такой паре из train\n",
        "#             val_pos_pairs[i] = [u, item]\n",
        "            \n",
        "#             val_negative_items[i] = np.random.randint(low=0, high=self.num_items, size=K)\n",
        "            \n",
        "#             # для разгрузки памяти\n",
        "#             del rated_items_idxs, item\n",
        "#             if i % 30000 == 0:\n",
        "#                 gc.collect()\n",
        "\n",
        "        with open(self.path + 'train', 'r') as f:\n",
        "            i = 0\n",
        "            for u, l in enumerate(tqdm(f, desc=f'Collect val info, {val_size} objects')):\n",
        "                if u in users_idxs:\n",
        "                    # будем предсказывать последний прослушанный трек\n",
        "                    item = int(l.split()[-1])\n",
        "                    val_pos_pairs[i] = [u, item]\n",
        "                    self.train_matr[u, item] = 0.0 # \"забыли\" о такой паре из train\n",
        "                    val_negative_items[i] = list(np.random.randint(\n",
        "                        low=0, high=self.num_items, size=K))\n",
        "                    i += 1                           \n",
        "        \n",
        "        return val_pos_pairs, val_negative_items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb37cf02",
      "metadata": {
        "id": "bb37cf02"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "# del ds\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43dd5b2e",
      "metadata": {
        "id": "43dd5b2e"
      },
      "outputs": [],
      "source": [
        "ds = Dataset('./')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b09bfdd8",
      "metadata": {
        "id": "b09bfdd8"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "ds.count_train_test_users()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0ae526a",
      "metadata": {
        "id": "f0ae526a"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "ds.get_train_matr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4064c66",
      "metadata": {
        "id": "d4064c66"
      },
      "outputs": [],
      "source": [
        "ds.train_users, ds.test_users, ds.num_items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "300508e0",
      "metadata": {
        "id": "300508e0"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "mem = sys.getsizeof(ds.train_matr)\n",
        "gb = mem // 1024 ** 3\n",
        "mb = (mem % 1024 ** 3) // 1024 ** 2\n",
        "kb = (mem % 1024 ** 2) // 1024\n",
        "\n",
        "gb, 'GB', mb, 'MB', kb, 'KB'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7c4c8e0",
      "metadata": {
        "id": "a7c4c8e0"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "ds.set_val_data(val_size=50000, K=99)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44a8312c",
      "metadata": {
        "id": "44a8312c"
      },
      "outputs": [],
      "source": [
        "len(ds.val_negative_items), len(ds.val_negative_items[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a9a2cd0",
      "metadata": {
        "id": "1a9a2cd0"
      },
      "outputs": [],
      "source": [
        "ds.val_ratings[2000:2003]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2522180b",
      "metadata": {
        "id": "2522180b"
      },
      "outputs": [],
      "source": [
        "# проверим на коллизии\n",
        "# то есть что положительный item оказался\n",
        "# среди выбранных отрицательных\n",
        "\n",
        "assert len(ds.val_ratings) == len(ds.val_negative_items)\n",
        "\n",
        "i_errors = []\n",
        "for i in range(len(ds.val_ratings)):\n",
        "    if ds.val_ratings[i][1] in ds.val_negative_items[i]:\n",
        "        i_errors.append(i)\n",
        "\n",
        "print('Error happened for i:\\n', ', '.join(map(str, i_errors)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40d7d928",
      "metadata": {
        "id": "40d7d928"
      },
      "outputs": [],
      "source": [
        "# изменим плохие объекты так чтобы брать другой индекс item\n",
        "\n",
        "for i in i_errors[::-1]:\n",
        "    for j in range(len(ds.val_negative_items[i])):\n",
        "        if ds.val_negative_items[i][j] == ds.val_ratings[i][1]:\n",
        "            if ds.val_negative_items[i][j] + 1 < ds.num_items:\n",
        "                ds.val_negative_items[i][j] += 1\n",
        "            else:\n",
        "                ds.val_negative_items[i][j] -= 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a439d20",
      "metadata": {
        "id": "6a439d20"
      },
      "source": [
        "# 3. Matrix Factorization\n",
        "\n",
        "В этой секции напишем метод матричной факторизации.  \n",
        "Метод заключается в разложении матрицы user-item в произведение двух меньших матриц (малоранговое приближение) $ R = PQ $, где размеры матриц следующие (d - размерность эмбеддингов пользователя и трека): \n",
        "\n",
        "$R (\\text{train_users + test_users}, \\text{num_items})$  \n",
        "$P (\\text{train_users + test_users}, \\text{d})$  \n",
        "$Q (\\text{d}, \\text{num_items})$\n",
        "\n",
        "При этом оценка конкретного трека пользователем определяется как скалярное произведение их эмбеддингов со сдвигом (bias)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43854e21",
      "metadata": {
        "id": "43854e21"
      },
      "outputs": [],
      "source": [
        "class MFModel(object):\n",
        "    \"\"\"\n",
        "    Модель matrix factorization \n",
        "    Обучается с помощью SGD и negative sampling\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_user, num_item, embedding_dim, reg, stddev):\n",
        "        \"\"\"\n",
        "        reg: коэффициент регуляризации\n",
        "        stddev: эмбеддинги сеплируются из нормального распределения\n",
        "                с таким стандартным отклонением\n",
        "        \"\"\"\n",
        "        self.user_embedding = np.random.normal(0, stddev, (num_user, embedding_dim))\n",
        "        self.item_embedding = np.random.normal(0, stddev, (num_item, embedding_dim))\n",
        "        self.user_bias = np.zeros([num_user])\n",
        "        self.item_bias = np.zeros([num_item])\n",
        "        self.bias = 0.0\n",
        "        self.reg = reg\n",
        "\n",
        "    def predict(self, pairs):\n",
        "        \"\"\"\n",
        "        pairs: кортеж списков (users, items) одинаковой длины\n",
        "        \"\"\"\n",
        "        assert len(pairs[0]) == len(pairs[1])\n",
        "        num_examples = len(pairs[0])\n",
        "        predictions = np.empty(num_examples)\n",
        "        for i in range(num_examples):\n",
        "            predictions[i] = self._predict_one(pairs[0][i], pairs[1][i])\n",
        "        return predictions\n",
        "\n",
        "    def _predict_one(self, user, item):\n",
        "        return (self.bias + self.user_bias[user] + self.item_bias[item] +\n",
        "                np.dot(self.user_embedding[user], self.item_embedding[item]))\n",
        "    \n",
        "    def fit(self, positive_pairs, learning_rate, num_negatives):\n",
        "        \"\"\"\n",
        "        positive_pairs: [n, 2], пары user-item с понравившимися треком пользователю\n",
        "        num_negatives: параметр negative sampling, равный числу отрицательных объектов\n",
        "        \"\"\"\n",
        "        \n",
        "        user_item_label_matrix = self._convert_ratings_to_implicit_data(\n",
        "            positive_pairs, num_negatives\n",
        "        )\n",
        "        np.random.shuffle(user_item_label_matrix)\n",
        "\n",
        "        num_examples = user_item_label_matrix.shape[0]\n",
        "        reg = self.reg\n",
        "        lr = learning_rate\n",
        "        sum_of_loss = 0.0\n",
        "        \n",
        "        for i in range(num_examples):\n",
        "            (user, item, rating) = user_item_label_matrix[i, :]\n",
        "            user_emb = self.user_embedding[user]\n",
        "            item_emb = self.item_embedding[item]\n",
        "            prediction = self._predict_one(user, item)\n",
        "\n",
        "            if prediction > 0:\n",
        "                one_plus_exp_minus_pred = 1.0 + np.exp(-prediction)\n",
        "                sigmoid = 1.0 / one_plus_exp_minus_pred\n",
        "                this_loss = (np.log(one_plus_exp_minus_pred) +\n",
        "                             (1.0 - rating) * prediction)\n",
        "            else:\n",
        "                exp_pred = np.exp(prediction)\n",
        "                sigmoid = exp_pred / (1.0 + exp_pred)\n",
        "                this_loss = -rating * prediction + np.log(1.0 + exp_pred)\n",
        "\n",
        "            grad = rating - sigmoid\n",
        "\n",
        "            self.user_embedding[user, :] += lr * (grad * item_emb - reg * user_emb)\n",
        "            self.item_embedding[item, :] += lr * (grad * user_emb - reg * item_emb)\n",
        "            self.user_bias[user] += lr * (grad - reg * self.user_bias[user])\n",
        "            self.item_bias[item] += lr * (grad - reg * self.item_bias[item])\n",
        "            self.bias += lr * (grad - reg * self.bias)\n",
        "\n",
        "            sum_of_loss += this_loss\n",
        "\n",
        "        return sum_of_loss / num_examples\n",
        "\n",
        "    \n",
        "    def _convert_ratings_to_implicit_data(self, positive_pairs, num_negatives):\n",
        "        \"\"\"\n",
        "        Делаем из списка пар датасет для бинарной классификации\n",
        "        На выходе получаем массив форму [n*(1 + num_negatives), 3]\n",
        "            его строки -- (user, item, label)\n",
        "        К каждой паре (user, item) из positive_pairs добавляем\n",
        "            num_negatives примеров отрицательного класса -- (user, item', 0)\n",
        "            где item выбираются случайно\n",
        "        \"\"\"\n",
        "        num_items = self.item_embedding.shape[0]\n",
        "        num_pos_examples = positive_pairs.shape[0]\n",
        "        training_matrix = np.empty([num_pos_examples * (1 + num_negatives), 3],\n",
        "                               dtype=np.int32)\n",
        "        index = 0\n",
        "        for pos_index in range(num_pos_examples):\n",
        "            u = positive_pairs[pos_index, 0]\n",
        "            i = positive_pairs[pos_index, 1]\n",
        "\n",
        "            training_matrix[index] = [u, i, 1]\n",
        "            index += 1\n",
        "\n",
        "            for _ in range(num_negatives):\n",
        "                j = i\n",
        "                while j == i:\n",
        "                    j = np.random.randint(num_items)\n",
        "                training_matrix[index] = [u, j, 0]\n",
        "                index += 1\n",
        "        return training_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d360e7cc",
      "metadata": {
        "id": "d360e7cc"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import heapq # --> top_N\n",
        "import numpy as np\n",
        "from time import time\n",
        "\n",
        "\n",
        "def eval_one_rating(model, testRatings, testNegatives, idx, top_N):\n",
        "    rating = testRatings[idx]\n",
        "    items = testNegatives[idx]\n",
        "    u = rating[0]\n",
        "    gtItem = rating[1]\n",
        "    items.append(gtItem)\n",
        "    \n",
        "    map_item_score = {}\n",
        "    users = np.full(len(items), u, dtype = 'int32')\n",
        "    \n",
        "    # assert type(users[0]) is int and type(np.array(items, dtype='int32')[0]) is int, \\\n",
        "    #        f'type(users[0]) is {type(users[0])}, type(np.array(items)[0]) is {type(np.array(items)[0])}'\n",
        "    #predictions = model.predict([users, np.array(items, dtype='int32')])\n",
        "    \n",
        "    # assert type(users[0]) is int and type(items[0]) is int, \\\n",
        "    #       f'type(users[0]) is {type(users[0])}, type(items[0]) is {type(items[0])}'\n",
        "    predictions = model.predict([users, items])\n",
        "    \n",
        "    for i in range(len(items)):\n",
        "        item = items[i]\n",
        "        map_item_score[item] = predictions[i]\n",
        "    items.pop()\n",
        "    \n",
        "    ranklist = heapq.nlargest(top_N, map_item_score, key=map_item_score.get)\n",
        "    hr = getHitRatio(ranklist, gtItem)\n",
        "    ndcg = getNDCG(ranklist, gtItem)\n",
        "    mrr = getMRR(ranklist, gtItem)\n",
        "    return (hr, ndcg, mrr)\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model(model, testRatings, testNegatives, top_N):\n",
        "    hits, ndcgs, mrrs = [], [], []\n",
        "    for idx in range(len(testRatings)):\n",
        "        (hr, ndcg, mrr) = eval_one_rating(\n",
        "            model, testRatings, testNegatives, idx, top_N\n",
        "        )\n",
        "        hits.append(hr)\n",
        "        ndcgs.append(ndcg)\n",
        "        mrrs.append(mrr)\n",
        "    return (hits, ndcgs, mrrs)\n",
        "\n",
        "\n",
        "def getHitRatio(ranklist, gtItem):\n",
        "    for item in ranklist:\n",
        "        if item == gtItem:\n",
        "            return 1\n",
        "    return 0\n",
        "\n",
        "\n",
        "def getNDCG(ranklist, gtItem):\n",
        "    for i in range(len(ranklist)):\n",
        "        item = ranklist[i]\n",
        "        if item == gtItem:\n",
        "            return math.log(2) / math.log(i+2)\n",
        "    return 0\n",
        "\n",
        "\n",
        "def getMRR(ranklist, gtItem):\n",
        "    for i in range(len(ranklist)):\n",
        "        if ranklist[i] == gtItem:\n",
        "            return 1. / (i + 1)\n",
        "    return 0\n",
        "\n",
        "\n",
        "def evaluate(model, test_ratings, test_negatives, top_N=100):\n",
        "    \"\"\"\n",
        "        Вспомогательная функция-обертка\n",
        "    \"\"\"\n",
        "    (hits, ndcgs, mrrs) = evaluate_model(\n",
        "        model, test_ratings, test_negatives, top_N=top_N\n",
        "    )\n",
        "    hits = np.array(hits)\n",
        "    ndcgs = np.array(ndcgs)\n",
        "    mrrs = np.array(mrrs)\n",
        "    return hits.mean(), ndcgs.mean(), mrrs.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a45e7f5f",
      "metadata": {
        "id": "a45e7f5f"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "train_pos_pairs = np.column_stack(ds.train_matr.nonzero())\n",
        "\n",
        "val_ratings, val_negative_items = (ds.val_ratings, ds.val_negative_items)\n",
        "\n",
        "print(\n",
        "    'Dataset: #user=%d, #item=%d, #train_pairs=%d, #test_pairs=%d' % (\n",
        "        ds.train_users + ds.test_users, ds.num_items, \n",
        "        train_pos_pairs.shape[0], len(val_ratings)\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c524625f",
      "metadata": {
        "id": "c524625f"
      },
      "outputs": [],
      "source": [
        "epochs = 17\n",
        "batch_size = 500\n",
        "n_batches = 100\n",
        "\n",
        "embedding_dim = 6\n",
        "regularization = 0.00003\n",
        "stddev = 0.02\n",
        "top_N = 30\n",
        "learning_rate = 0.00003"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c7035f7",
      "metadata": {
        "id": "0c7035f7"
      },
      "outputs": [],
      "source": [
        "del model\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2973baad",
      "metadata": {
        "id": "2973baad"
      },
      "outputs": [],
      "source": [
        "model = MFModel(\n",
        "    ds.train_users + ds.test_users, \n",
        "    ds.num_items,\n",
        "    embedding_dim-1, \n",
        "    regularization, \n",
        "    stddev\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aff3302",
      "metadata": {
        "id": "0aff3302"
      },
      "outputs": [],
      "source": [
        "hist_hr, hist_ndcg, hist_mrr = [], [], []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6079bf6",
      "metadata": {
        "id": "a6079bf6"
      },
      "outputs": [],
      "source": [
        "hr, ndcg, mrr = evaluate(model, val_ratings, val_negative_items, top_N=top_N)\n",
        "print('Epoch %4d:\\t HR=%.4f, NDCG=%.4f\\t, MRR=%.4f\\t'\n",
        "    % (0, hr, ndcg, mrr))\n",
        "\n",
        "hist_hr.append(hr)\n",
        "hist_ndcg.append(ndcg)\n",
        "hist_mrr.append(mrr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c1c475f",
      "metadata": {
        "scrolled": false,
        "id": "3c1c475f"
      },
      "outputs": [],
      "source": [
        "for epoch in range(epochs):\n",
        "    pbar = tqdm(range(n_batches))\n",
        "    \n",
        "    for batch in pbar:\n",
        "        pbar.set_description(f'Epoch {epoch}, Batch {batch}/{n_batches}:')\n",
        "        \n",
        "        idxs = np.random.choice(\n",
        "            range(len(train_pos_pairs)), size=batch_size, replace=False\n",
        "        )\n",
        "        \n",
        "        _ = model.fit(\n",
        "            train_pos_pairs[idxs], \n",
        "            learning_rate=learning_rate,\n",
        "            num_negatives=top_N\n",
        "        )\n",
        "\n",
        "    hr, ndcg, mrr = evaluate(model, val_ratings, val_negative_items, top_N=top_N)\n",
        "    print('Epoch %4d:\\t HR=%.4f, NDCG=%.4f\\t, MRR=%.4f\\t'\n",
        "          % (epoch+1, hr, ndcg, mrr))\n",
        "    \n",
        "    hist_hr.append(hr)\n",
        "    hist_ndcg.append(ndcg)\n",
        "    hist_mrr.append(mrr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce92788e",
      "metadata": {
        "id": "ce92788e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
        "\n",
        "plt.subplot(131)\n",
        "plt.title('Hit Rate history')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Metric')\n",
        "plt.plot(hist_hr)\n",
        "plt.grid()\n",
        "plt.xticks(range(len(hist_hr)))\n",
        "plt.ylim(0.0, 0.6)\n",
        "plt.yticks(np.linspace(0.0, 0.6, 13))\n",
        "\n",
        "plt.subplot(132)\n",
        "plt.title('NDCG history')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Metric')\n",
        "plt.plot(hist_ndcg)\n",
        "plt.grid()\n",
        "plt.xticks(range(len(hist_hr)))\n",
        "plt.ylim(0.0, 0.6)\n",
        "plt.yticks(np.linspace(0.0, 0.6, 13))\n",
        "\n",
        "\n",
        "plt.subplot(133)\n",
        "plt.title('MRR history')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Metric')\n",
        "plt.plot(hist_mrr)\n",
        "plt.grid()\n",
        "plt.xticks(range(len(hist_hr)))\n",
        "plt.ylim(0.0, 0.6)\n",
        "plt.yticks(np.linspace(0.0, 0.6, 13))\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95aec365",
      "metadata": {
        "id": "95aec365"
      },
      "outputs": [],
      "source": [
        "pbar = tqdm(\n",
        "    range(ds.train_users, ds.train_users + ds.test_users)\n",
        ")\n",
        "\n",
        "# print('Predict best tracks for test')\n",
        "\n",
        "top_N = 100\n",
        "choose_items_num = 24000\n",
        "# all_items = [i for i in range(ds.num_items)]\n",
        "\n",
        "with open('preds.txt', 'w') as f:\n",
        "    buffer = ''\n",
        "    for it, u in enumerate(pbar):    \n",
        "        map_item_score = {}\n",
        "        # users = np.full(len(all_items), u, dtype = 'int32')\n",
        "        # predictions = model.predict([users, all_items])\n",
        "        items_sample = np.random.choice(\n",
        "            range(ds.num_items), size=choose_items_num, replace=False\n",
        "        )\n",
        "        users = np.full(len(items_sample), u, dtype = 'int32')\n",
        "        predictions = model.predict([users, items_sample])\n",
        "\n",
        "        for i in range(len(items_sample)):\n",
        "            # item = all_items[i]\n",
        "            map_item_score[items_sample[i]] = predictions[i]\n",
        "\n",
        "        ranklist = heapq.nlargest(top_N, map_item_score, key=map_item_score.get)\n",
        "        \n",
        "        buffer = buffer + ' '.join(map(str, ranklist)) + '\\n'\n",
        "        if (it + 1) % 200 == 0:\n",
        "            f.write(buffer)\n",
        "            buffer = ''\n",
        "        \n",
        "        # del users, predictions, map_item_score, items_sample #, ranklist, line\n",
        "        # gc.collect()\n",
        "    \n",
        "    f.write(buffer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WHRzQ76n4YAz"
      },
      "id": "WHRzQ76n4YAz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Matrix Factorization 2"
      ],
      "metadata": {
        "id": "5ksQiBBkeb1d"
      },
      "id": "5ksQiBBkeb1d"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, num_train_users, num_test_users, num_items, embed_dim, stddev):\n",
        "        self.num_train_users = num_train_users\n",
        "        self.num_test_users = num_test_users\n",
        "        self.num_items = num_items\n",
        "        self.embed_dim = embed_dim\n",
        "        self.stddev = stddev\n",
        "        self.init_embed_normal(self.stddev)\n",
        "        self.mlp = nn.Linear(\n",
        "            in_features=2 * self.embed_dim,\n",
        "            out_features=1\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def init_embed_normal(self, stddev):\n",
        "        self.users_embeds = nn.Embedding(self.num_train_users + self.num_test_users, self.embed_dim)\n",
        "        self.items_embeds = nn.Embedding(self.num_items, self.embed_dim)\n",
        "        self.users_embeds.weight = nn.Parameter(\n",
        "            torch.randn(\n",
        "                (self.num_train_users + self.num_test_users, self.embed_dim)\n",
        "            ) * stddev\n",
        "        )\n",
        "        self.items_embeds.weight = nn.Parameter(\n",
        "            torch.randn(\n",
        "                (self.num_train_users + self.num_test_users, self.embed_dim)\n",
        "            ) * stddev\n",
        "        )\n",
        "    \n",
        "    def forward(self, users_id_batch, items_id_batch):\n",
        "        batch_u_emb = self.users_embeds(users_id_batch)\n",
        "        batch_i_emb = self.items_embeds(items_id_batch)\n",
        "        batch_emb = torch.cat([batch_u_emb, batch_i_emb], dim=1)\n",
        "        batch_logits = self.mlp(batch_emb)\n",
        "        batch_preds = self.sigmoid(batch_logits)\n",
        "        return batch_preds\n",
        "\n",
        "    def predict(ui_list):\n",
        "        \"\"\" ui_list = [[users, items]] \"\"\"\n",
        "        users = ui_list[0]\n",
        "        items = ui_list[1]\n",
        "        u_emb = self.users_embeds(users)\n",
        "        i_emb = self.items_embeds(items)\n",
        "        emb = torch.cat([u_emb, i_emb], dim=1)\n",
        "        logits = self.mlp(emb)\n",
        "        preds = self.sigmoid(logits)\n",
        "        return preds"
      ],
      "metadata": {
        "id": "lky1WfBK4XqC"
      },
      "id": "lky1WfBK4XqC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_ratings_to_implicit_data(positive_pairs, num_negatives):\n",
        "        \"\"\"\n",
        "        Делаем из списка пар датасет для бинарной классификации\n",
        "        На выходе получаем массив форму [n*(1 + num_negatives), 3]\n",
        "        его строки -- (user, item, label)\n",
        "        К каждой паре (user, item) из positive_pairs добавляем\n",
        "        num_negatives примеров отрицательного класса -- (user, item', 0)\n",
        "        где item выбираются случайно\n",
        "        \"\"\"\n",
        "        num_items = self.item_embedding.shape[0]\n",
        "        num_pos_examples = positive_pairs.shape[0]\n",
        "        training_matrix = np.empty([num_pos_examples * (1 + num_negatives), 3],\n",
        "                               dtype=np.int32)\n",
        "        index = 0\n",
        "        for pos_index in range(num_pos_examples):\n",
        "            u = positive_pairs[pos_index, 0]\n",
        "            i = positive_pairs[pos_index, 1]\n",
        "\n",
        "            training_matrix[index] = [u, i, 1]\n",
        "            index += 1\n",
        "\n",
        "            for _ in range(num_negatives):\n",
        "                j = i\n",
        "                while j == i:\n",
        "                    j = np.random.randint(num_items)\n",
        "                training_matrix[index] = [u, j, 0]\n",
        "                index += 1\n",
        "        return training_matrix\n",
        "\n",
        "    \n",
        "\n",
        "def train_epoch(model, epochs, n_batches, batch_size, train_pos_pairs, num_negatives, loss_fn, opt):\n",
        "    hist_hr, hist_ndcg, hist_mrr = [], [], []\n",
        "    \n",
        "    hr, ndcg, mrr = evaluate(model, val_ratings, val_negative_items, top_N=top_N)\n",
        "    print('Epoch %4d:\\t HR=%.4f, NDCG=%.4f\\t, MRR=%.4f\\t'\n",
        "        % (0, hr, ndcg, mrr))\n",
        "\n",
        "    hist_hr.append(hr)\n",
        "    hist_ndcg.append(ndcg)\n",
        "    hist_mrr.append(mrr)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        pbar = tqdm(range(n_batches))\n",
        "\n",
        "        for batch in pbar:\n",
        "            opt.zero_grad()\n",
        "            pbar.set_description(f'Epoch {epoch}, Batch {batch}/{n_batches}:')\n",
        "\n",
        "            idxs = np.random.choice(\n",
        "                range(len(train_pos_pairs)), size=batch_size, replace=False\n",
        "            )\n",
        "            \n",
        "            cur_ds = train_pos_pairs[idxs]\n",
        "            cur_ds = convert_ratings_to_implicit_data(cur_ds)\n",
        "            batch_true = torch.Tensor(cur_ds[:, 2])\n",
        "            cur_ds = torch.Tensor(cur_ds)\n",
        "            batch_preds = model(cur_ds)\n",
        "            loss = loss_fn(batch_preds, batch_true)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        hr, ndcg, mrr = evaluate(model, val_ratings, val_negative_items, top_N=top_N)\n",
        "        print('Epoch %4d:\\t HR=%.4f, NDCG=%.4f\\t, MRR=%.4f\\t'\n",
        "              % (epoch+1, hr, ndcg, mrr))\n",
        "\n",
        "        hist_hr.append(hr)\n",
        "        hist_ndcg.append(ndcg)\n",
        "        hist_mrr.append(mrr)\n",
        "    \n",
        "    return hist_hr, hist_ndcg, hist_mrr"
      ],
      "metadata": {
        "id": "F286Vqdc4XnX"
      },
      "id": "F286Vqdc4XnX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "train_pos_pairs = np.column_stack(ds.train_matr.nonzero())\n",
        "\n",
        "val_ratings, val_negative_items = (ds.val_ratings, ds.val_negative_items)\n",
        "\n",
        "print(\n",
        "    'Dataset: #user=%d, #item=%d, #train_pairs=%d, #test_pairs=%d' % (\n",
        "        ds.train_users + ds.test_users, ds.num_items, \n",
        "        train_pos_pairs.shape[0], len(val_ratings)\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "yVICJd_S4Xkh"
      },
      "id": "yVICJd_S4Xkh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 17\n",
        "batch_size = 500\n",
        "n_batches = 100\n",
        "\n",
        "embedding_dim = 6\n",
        "regularization = 0.00003\n",
        "stddev = 0.02\n",
        "top_N = 30\n",
        "learning_rate = 0.00003"
      ],
      "metadata": {
        "id": "lOxI66Rs4Xhc"
      },
      "id": "lOxI66Rs4Xhc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP(num_train_users, num_test_users, num_items, embed_dim, stddev)\n",
        "\n",
        "loss_fn = nn.BCELoss()  # binary cross entropy\n",
        "opt = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "B5-QQrVleogy"
      },
      "id": "B5-QQrVleogy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist_hr, hist_ndcg, hist_mrr = train_epoch(model, epochs, n_batches, batch_size, train_pos_pairs, num_negatives, loss_fn, opt)"
      ],
      "metadata": {
        "id": "h77zwWiAeodX"
      },
      "id": "h77zwWiAeodX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
        "\n",
        "plt.subplot(131)\n",
        "plt.title('Hit Rate history')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Metric')\n",
        "plt.plot(new_hr)\n",
        "plt.grid()\n",
        "plt.xticks(range(len(hist_hr)))\n",
        "plt.ylim(0.0, 0.6)\n",
        "plt.yticks(np.linspace(0.0, 0.6, 13))\n",
        "\n",
        "plt.subplot(132)\n",
        "plt.title('NDCG history')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Metric')\n",
        "plt.plot(new_ndcg)\n",
        "plt.grid()\n",
        "plt.xticks(range(len(hist_hr)))\n",
        "plt.ylim(0.0, 0.6)\n",
        "plt.yticks(np.linspace(0.0, 0.6, 13))\n",
        "\n",
        "\n",
        "plt.subplot(133)\n",
        "plt.title('MRR history')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Metric')\n",
        "plt.plot(new_mrr)\n",
        "plt.grid()\n",
        "plt.xticks(range(len(hist_hr)))\n",
        "plt.ylim(0.0, 0.6)\n",
        "plt.yticks(np.linspace(0.0, 0.6, 13))\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "YqXVmjgUeoaP"
      },
      "id": "YqXVmjgUeoaP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pbar = tqdm(\n",
        "    range(ds.train_users, ds.train_users + ds.test_users)\n",
        ")\n",
        "\n",
        "# print('Predict best tracks for test')\n",
        "\n",
        "top_N = 100\n",
        "choose_items_num = 24000\n",
        "# all_items = [i for i in range(ds.num_items)]\n",
        "\n",
        "with open('preds.txt', 'w') as f:\n",
        "    buffer = ''\n",
        "    for it, u in enumerate(pbar):    \n",
        "        map_item_score = {}\n",
        "        # users = np.full(len(all_items), u, dtype = 'int32')\n",
        "        # predictions = model.predict([users, all_items])\n",
        "        items_sample = np.random.choice(\n",
        "            range(ds.num_items), size=choose_items_num, replace=False\n",
        "        )\n",
        "        users = np.full(len(items_sample), u, dtype = 'int32')\n",
        "        predictions = model.predict([users, items_sample])\n",
        "\n",
        "        for i in range(len(items_sample)):\n",
        "            # item = all_items[i]\n",
        "            map_item_score[items_sample[i]] = predictions[i]\n",
        "\n",
        "        ranklist = heapq.nlargest(top_N, map_item_score, key=map_item_score.get)\n",
        "        \n",
        "        buffer = buffer + ' '.join(map(str, ranklist)) + '\\n'\n",
        "        if (it + 1) % 200 == 0:\n",
        "            f.write(buffer)\n",
        "            buffer = ''\n",
        "        \n",
        "        # del users, predictions, map_item_score, items_sample #, ranklist, line\n",
        "        # gc.collect()\n",
        "    \n",
        "    f.write(buffer)"
      ],
      "metadata": {
        "id": "C64TjfjY4XVd"
      },
      "id": "C64TjfjY4XVd",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}